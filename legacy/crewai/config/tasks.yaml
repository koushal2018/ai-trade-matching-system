# Optimized Task Descriptions - 83% Token Reduction
# Following scratchpad pattern: agents save details to S3, pass only summaries

document_processing_task:
  description: >
    Convert PDF at {document_path} to JPEG images (300 DPI).
    Save to S3: {s3_bucket}/PDFIMAGES/{source_type}/{unique_identifier}/ and
    locally to /tmp/processing/{unique_identifier}/pdf_images/.
    Use 3-digit padding: page_001.jpg, page_002.jpg, etc.

  expected_output: >
    "Images saved to S3: {s3_bucket}/PDFIMAGES/{source_type}/{unique_identifier}/.
    Local path: /tmp/processing/{unique_identifier}/pdf_images/. Pages: [X]"

  agent: document_processor

ocr_processing_task:
  description: >
    Extract text from ALL PDF images using OCR and save to local file. Follow these steps:

    STEP 1 - List Image Files:
    Use "List files in directory" on /tmp/processing/{unique_identifier}/pdf_images/
    Find all .jpg files (page_001.jpg, page_002.jpg, etc.)

    STEP 2 - Extract Text from EACH Image:
    Use "Optical Character Recognition Tool" on EACH image file found.
    Important: Process ALL images in the directory (typically 5 pages).

    STEP 3 - Combine All Text:
    Concatenate text from all pages into one document.
    Separate each page with "--- PAGE X ---" header.

    STEP 4 - Save Combined Text Locally:
    Use "File Writer Tool" to save combined text to:
    Filename: "ocr_text.txt"
    Directory: "/tmp/processing/{unique_identifier}/"
    Overwrite: "true"

    STEP 5 - Return Confirmation:
    Final answer: "OCR complete. Text saved to /tmp/processing/{unique_identifier}/ocr_text.txt. Pages processed: [X]"

  expected_output: >
    "OCR complete. Text saved to /tmp/processing/{unique_identifier}/ocr_text.txt. Pages processed: [X]"

  agent: ocr_processor

trade_entity_extractor_task:
  description: >
    Read OCR text, parse into structured JSON, and save to S3. Follow these steps:

    STEP 1 - Read OCR Text:
    Use "Read a file's content" tool to read /tmp/processing/{unique_identifier}/ocr_text.txt
    This contains all extracted text from the PDF.

    STEP 2 - Parse Trade Data:
    Extract these fields from the text into JSON format:
    - Trade_ID (required)
    - TRADE_SOURCE (must be "{source_type}")
    - counterparty, trade_date, effective_date, maturity_date
    - notional, currency, commodity_type, product_type
    - Any other relevant trade details

    STEP 3 - Save to S3:
    Use "S3 Writer Tool" with:
    file_path: "s3://{s3_bucket}/extracted/{source_type}/trade_{unique_identifier}_{timestamp}.json"
    content: Your JSON string (properly formatted)

    STEP 4 - Return S3 Path:
    Final answer MUST be EXACTLY:
    "S3_PATH: s3://{s3_bucket}/extracted/{source_type}/trade_{unique_identifier}_{timestamp}.json TRADE_SOURCE: {source_type}"

    DO NOT include JSON content. ONLY the S3_PATH line.

  expected_output: >
    "S3_PATH: s3://otc-menat-2025/extracted/{source_type}/trade_{unique_identifier}_{timestamp}.json TRADE_SOURCE: {source_type}"

  agent: trade_entity_extractor

reporting_task:
  description: >
    Store extracted trade data in DynamoDB. Follow these steps IN ORDER:

    STEP 1 - Extract S3 Path from Context:
    The previous task output contains a line like: "S3_PATH: s3://otc-menat-2025/extracted/..."
    Find and extract this EXACT S3 path from the context.

    ERROR HANDLING: If you cannot find "S3_PATH:" in the previous output:
    - Look for any s3:// path in the context
    - Or check /tmp/processing/{unique_identifier}/ for local JSON files
    - Report error if no data source found

    STEP 2 - Read Trade JSON:
    Use "S3 Reader Tool" with the S3 path you extracted.
    This will return JSON with trade data including Trade_ID and TRADE_SOURCE.

    STEP 3 - Determine Target Table:
    Read TRADE_SOURCE field from JSON:
    - If TRADE_SOURCE = "BANK" → use table {dynamodb_bank_table}
    - If TRADE_SOURCE = "COUNTERPARTY" → use table {dynamodb_counterparty_table}
    - If TRADE_SOURCE is anything else → report DATA ERROR

    STEP 4 - Store in DynamoDB:
    Use MCP DynamoDB "put_item" tool. ALL fields must have type markers:
    Example format:
    {{
      "Trade_ID": {{"S": "GCS382857"}},
      "TRADE_SOURCE": {{"S": "COUNTERPARTY"}},
      "notional": {{"N": "1000000"}},
      "s3_source": {{"S": "s3://bucket/path/file.json"}},
      "processing_timestamp": {{"S": "2025-10-11T22:29:34Z"}}
    }}

    STEP 5 - Confirm Storage:
    Return: "Stored trade [Trade_ID] in table [table_name]. S3 source: [s3_path]"

  expected_output: >
    "Stored trade [Trade_ID] in table [table_name]. S3 source: [s3_path]"

  agent: reporting_analyst

matching_task:
  description: >
    Perform trade matching between bank and counterparty data. Follow these steps IN ORDER:

    STEP 1 - Data Integrity Check:
    Use MCP DynamoDB "scan" tool to retrieve all records from both tables:
    - Scan {dynamodb_bank_table} - verify ALL records have TRADE_SOURCE = "BANK"
    - Scan {dynamodb_counterparty_table} - verify ALL records have TRADE_SOURCE = "COUNTERPARTY"
    If any records have wrong TRADE_SOURCE, classify as DATA_ERROR.

    STEP 2 - Perform Matching:
    For each COUNTERPARTY trade, find matching BANK trade using these criteria:
    - Trade_ID: Must match exactly
    - Trade_Date: Within ±1 business day tolerance
    - Notional: Within ±0.01% tolerance
    - Counterparty: Fuzzy match (allow minor spelling differences)

    STEP 3 - Classify Results:
    - MATCHED: All criteria match within tolerances
    - PROBABLE_MATCH: Trade_ID + 2 of 3 other fields match
    - REVIEW_REQUIRED: Trade_ID matches but differences in other fields
    - BREAK: No matching Trade_ID found
    - DATA_ERROR: Wrong TRADE_SOURCE in wrong table

    STEP 4 - Generate Report:
    Create markdown report with:
    - Summary statistics (match rate, breaks, reviews needed)
    - List of matched trades
    - List of breaks requiring attention
    - Data errors found
    Use "S3 Writer Tool" to save to: s3://{s3_bucket}/reports/matching_report_{unique_identifier}_{timestamp}.md

    STEP 5 - Return Summary:
    "Report saved to s3://{s3_bucket}/reports/matching_report_{unique_identifier}_{timestamp}.md. Match rate: [X%]. Breaks: [Y]. Reviews needed: [Z]"

  expected_output: >
    "Report saved to {s3_bucket}/reports/matching_report_{unique_identifier}_{timestamp}.md.
    Match rate: [X%]. Breaks: [Y]. Reviews needed: [Z]"

  agent: matching_analyst
